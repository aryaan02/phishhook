{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVa8R4nWyAy_"
      },
      "source": [
        "# PhishHook - A Phishing URL Detector\n",
        "#### By: Aryaan Khan and Bradley Lewis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iZYwNqUydlU"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "05wY1ux9x9D7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from urllib.parse import urlparse, parse_qs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-NbEyT20IkA"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "n-7y7lzg0HsJ",
        "outputId": "52a1eba3-5f02-49e9-e161-31328f9b1f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11481 entries, 0 to 11480\n",
            "Data columns (total 24 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   length_url       11481 non-null  float64\n",
            " 1   length_hostname  11481 non-null  float64\n",
            " 2   ip               11481 non-null  float64\n",
            " 3   nb_dots          11481 non-null  float64\n",
            " 4   nb_hyphens       11481 non-null  float64\n",
            " 5   nb_at            11481 non-null  float64\n",
            " 6   nb_qm            11481 non-null  float64\n",
            " 7   nb_and           11481 non-null  float64\n",
            " 8   nb_or            11481 non-null  float64\n",
            " 9   nb_eq            11481 non-null  float64\n",
            " 10  nb_underscore    11481 non-null  float64\n",
            " 11  nb_tilde         11481 non-null  float64\n",
            " 12  nb_percent       11481 non-null  float64\n",
            " 13  nb_slash         11481 non-null  float64\n",
            " 14  nb_star          11481 non-null  float64\n",
            " 15  nb_colon         11481 non-null  float64\n",
            " 16  nb_comma         11481 non-null  float64\n",
            " 17  nb_semicolumn    11481 non-null  float64\n",
            " 18  nb_dollar        11481 non-null  float64\n",
            " 19  nb_space         11481 non-null  float64\n",
            " 20  nb_www           11481 non-null  float64\n",
            " 21  nb_com           11481 non-null  float64\n",
            " 22  nb_dslash        11481 non-null  float64\n",
            " 23  status           11481 non-null  float64\n",
            "dtypes: float64(24)\n",
            "memory usage: 2.1 MB\n"
          ]
        }
      ],
      "source": [
        "# Define the features to keep, based on the extract_url_features function output\n",
        "features_to_keep = [\n",
        "    'length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', \n",
        "    'nb_qm', 'nb_and', 'nb_or', 'nb_eq', 'nb_underscore', 'nb_tilde', 'nb_percent', \n",
        "    'nb_slash', 'nb_star', 'nb_colon', 'nb_comma', 'nb_semicolumn', 'nb_dollar', \n",
        "    'nb_space', 'nb_www', 'nb_com', 'nb_dslash', 'status'  # 'status' is the target variable\n",
        "]\n",
        "\n",
        "print(len(features_to_keep))\n",
        "\n",
        "# Read the data from the CSV file\n",
        "phishing_data = pd.read_csv(\"phishing_data.csv\")\n",
        "\n",
        "# Drop columns not in the features_to_keep list\n",
        "phishing_data = phishing_data[features_to_keep]\n",
        "\n",
        "# Replace string values with corresponding integer for columns known to require it (if any)\n",
        "# This step may need to be adjusted based on the actual data values in these columns\n",
        "phishing_data['nb_hyphens'] = phishing_data['nb_hyphens'].replace({'zero': 0, 'one': 1}).astype(int)\n",
        "\n",
        "# Convert the target values to binary\n",
        "phishing_data['status'] = (phishing_data['status'] == 'phishing').astype(int)\n",
        "\n",
        "# Check for any other columns that might have inconsistent types and convert them\n",
        "for column in phishing_data.columns:\n",
        "    if phishing_data[column].dtype  == 'object':\n",
        "        phishing_data[column] = pd.to_numeric(phishing_data[column], errors='coerce')\n",
        "\n",
        "# Fill NA values with the mean of each column\n",
        "phishing_data.fillna(phishing_data.mean(), inplace=True)\n",
        "\n",
        "# Convert all columns to float64\n",
        "phishing_data = phishing_data.astype(float)\n",
        "\n",
        "# Display the info to confirm all types are now float64 and only desired columns are retained\n",
        "phishing_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR3DtFf12Ize"
      },
      "source": [
        "## Create the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "_J6ug_ZA2LXF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([9184, 23]), torch.Size([9184, 1]))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split the data into features and target variable\n",
        "X = phishing_data.drop('status', axis=1).values\n",
        "y = phishing_data['status'].values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the datasets into PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Create Tensor datasets\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Confirm the DataLoader details\n",
        "train_loader.dataset.tensors[0].shape, train_loader.dataset.tensors[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo8eJ1Vbyff1"
      },
      "source": [
        "## Define the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "j-XPOZBIyfIb"
      },
      "outputs": [],
      "source": [
        "class PhishHookNet(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(PhishHookNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 128)\n",
        "        # self.dropout1 = nn.Dropout(0.5)\n",
        "        self.layer2 = nn.Linear(128, 64)\n",
        "        # self.dropout2 = nn.Dropout(0.5)\n",
        "        self.output = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        # x = self.dropout1(x)\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        # x = self.dropout2(x)\n",
        "        x = torch.sigmoid(self.output(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBNkBiOgyqpW"
      },
      "source": [
        "## Define the Training and Validation Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "3MsmK3vP2ggw"
      },
      "outputs": [],
      "source": [
        "# Function to train the model\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "# Function to validate the model\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "    avg_loss = total_loss / len(val_loader.dataset)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Early Stopping Condition Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'phishing_url_model.pth')\n",
        "        self.val_loss_min = val_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgfYAE0822ML"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Bkekk9wt21hg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss decreased (inf --> 0.544209).  Saving model ...\n",
            "Epoch 1: train_loss = 0.6174, val_loss = 0.5442\n",
            "Validation loss decreased (0.544209 --> 0.482140).  Saving model ...\n",
            "Epoch 2: train_loss = 0.5059, val_loss = 0.4821\n",
            "Validation loss decreased (0.482140 --> 0.423185).  Saving model ...\n",
            "Epoch 3: train_loss = 0.4652, val_loss = 0.4232\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 4: train_loss = 0.4489, val_loss = 0.4281\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 5: train_loss = 0.4496, val_loss = 0.4353\n",
            "EarlyStopping counter: 3 out of 15\n",
            "Epoch 6: train_loss = 0.4376, val_loss = 0.4360\n",
            "Validation loss decreased (0.423185 --> 0.411403).  Saving model ...\n",
            "Epoch 7: train_loss = 0.4345, val_loss = 0.4114\n",
            "Validation loss decreased (0.411403 --> 0.407022).  Saving model ...\n",
            "Epoch 8: train_loss = 0.4396, val_loss = 0.4070\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 9: train_loss = 0.4346, val_loss = 0.4639\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 10: train_loss = 0.4379, val_loss = 0.4341\n",
            "EarlyStopping counter: 3 out of 15\n",
            "Epoch 11: train_loss = 0.4305, val_loss = 0.4102\n",
            "EarlyStopping counter: 4 out of 15\n",
            "Epoch 12: train_loss = 0.4376, val_loss = 0.4076\n",
            "EarlyStopping counter: 5 out of 15\n",
            "Epoch 13: train_loss = 0.4345, val_loss = 0.4176\n",
            "Validation loss decreased (0.407022 --> 0.401402).  Saving model ...\n",
            "Epoch 14: train_loss = 0.4368, val_loss = 0.4014\n",
            "Validation loss decreased (0.401402 --> 0.399617).  Saving model ...\n",
            "Epoch 15: train_loss = 0.4274, val_loss = 0.3996\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 16: train_loss = 0.4291, val_loss = 0.4026\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 17: train_loss = 0.4339, val_loss = 0.5035\n",
            "EarlyStopping counter: 3 out of 15\n",
            "Epoch 18: train_loss = 0.4224, val_loss = 0.4010\n",
            "EarlyStopping counter: 4 out of 15\n",
            "Epoch 19: train_loss = 0.4227, val_loss = 0.4202\n",
            "EarlyStopping counter: 5 out of 15\n",
            "Epoch 20: train_loss = 0.4208, val_loss = 0.4004\n",
            "EarlyStopping counter: 6 out of 15\n",
            "Epoch 21: train_loss = 0.4187, val_loss = 0.4164\n",
            "EarlyStopping counter: 7 out of 15\n",
            "Epoch 22: train_loss = 0.4250, val_loss = 0.4415\n",
            "EarlyStopping counter: 8 out of 15\n",
            "Epoch 23: train_loss = 0.4211, val_loss = 0.4047\n",
            "Validation loss decreased (0.399617 --> 0.398071).  Saving model ...\n",
            "Epoch 24: train_loss = 0.4201, val_loss = 0.3981\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 25: train_loss = 0.4182, val_loss = 0.3989\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 26: train_loss = 0.4142, val_loss = 0.4060\n",
            "Validation loss decreased (0.398071 --> 0.392854).  Saving model ...\n",
            "Epoch 27: train_loss = 0.4199, val_loss = 0.3929\n",
            "Validation loss decreased (0.392854 --> 0.391300).  Saving model ...\n",
            "Epoch 28: train_loss = 0.4127, val_loss = 0.3913\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 29: train_loss = 0.4093, val_loss = 0.3995\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 30: train_loss = 0.4128, val_loss = 0.4027\n",
            "Validation loss decreased (0.391300 --> 0.386619).  Saving model ...\n",
            "Epoch 31: train_loss = 0.4076, val_loss = 0.3866\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 32: train_loss = 0.4117, val_loss = 0.3868\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 33: train_loss = 0.4095, val_loss = 0.3936\n",
            "EarlyStopping counter: 3 out of 15\n",
            "Epoch 34: train_loss = 0.4116, val_loss = 0.3902\n",
            "EarlyStopping counter: 4 out of 15\n",
            "Epoch 35: train_loss = 0.4126, val_loss = 0.4020\n",
            "EarlyStopping counter: 5 out of 15\n",
            "Epoch 36: train_loss = 0.4062, val_loss = 0.3934\n",
            "EarlyStopping counter: 6 out of 15\n",
            "Epoch 37: train_loss = 0.4093, val_loss = 0.3980\n",
            "EarlyStopping counter: 7 out of 15\n",
            "Epoch 38: train_loss = 0.4094, val_loss = 0.4036\n",
            "EarlyStopping counter: 8 out of 15\n",
            "Epoch 39: train_loss = 0.4095, val_loss = 0.3869\n",
            "Validation loss decreased (0.386619 --> 0.386556).  Saving model ...\n",
            "Epoch 40: train_loss = 0.4059, val_loss = 0.3866\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 41: train_loss = 0.4023, val_loss = 0.3890\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 42: train_loss = 0.4099, val_loss = 0.3947\n",
            "Validation loss decreased (0.386556 --> 0.384604).  Saving model ...\n",
            "Epoch 43: train_loss = 0.4052, val_loss = 0.3846\n",
            "Validation loss decreased (0.384604 --> 0.382492).  Saving model ...\n",
            "Epoch 44: train_loss = 0.4035, val_loss = 0.3825\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 45: train_loss = 0.4022, val_loss = 0.4058\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 46: train_loss = 0.4068, val_loss = 0.3860\n",
            "EarlyStopping counter: 3 out of 15\n",
            "Epoch 47: train_loss = 0.3996, val_loss = 0.3857\n",
            "EarlyStopping counter: 4 out of 15\n",
            "Epoch 48: train_loss = 0.4000, val_loss = 0.3901\n",
            "EarlyStopping counter: 5 out of 15\n",
            "Epoch 49: train_loss = 0.3995, val_loss = 0.3841\n",
            "EarlyStopping counter: 6 out of 15\n",
            "Epoch 50: train_loss = 0.4046, val_loss = 0.3885\n",
            "EarlyStopping counter: 7 out of 15\n",
            "Epoch 51: train_loss = 0.3997, val_loss = 0.3885\n",
            "EarlyStopping counter: 8 out of 15\n",
            "Epoch 52: train_loss = 0.4045, val_loss = 0.3830\n",
            "EarlyStopping counter: 9 out of 15\n",
            "Epoch 53: train_loss = 0.4020, val_loss = 0.3909\n",
            "EarlyStopping counter: 10 out of 15\n",
            "Epoch 54: train_loss = 0.3987, val_loss = 0.3885\n",
            "EarlyStopping counter: 11 out of 15\n",
            "Epoch 55: train_loss = 0.4044, val_loss = 0.3873\n",
            "EarlyStopping counter: 12 out of 15\n",
            "Epoch 56: train_loss = 0.4032, val_loss = 0.3884\n",
            "EarlyStopping counter: 13 out of 15\n",
            "Epoch 57: train_loss = 0.3978, val_loss = 0.3829\n",
            "Validation loss decreased (0.382492 --> 0.382395).  Saving model ...\n",
            "Epoch 58: train_loss = 0.4048, val_loss = 0.3824\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 59: train_loss = 0.3969, val_loss = 0.3832\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 60: train_loss = 0.3997, val_loss = 0.3846\n",
            "EarlyStopping counter: 3 out of 15\n",
            "Epoch 61: train_loss = 0.3952, val_loss = 0.4151\n",
            "EarlyStopping counter: 4 out of 15\n",
            "Epoch 62: train_loss = 0.4007, val_loss = 0.3831\n",
            "EarlyStopping counter: 5 out of 15\n",
            "Epoch 63: train_loss = 0.3999, val_loss = 0.3871\n",
            "EarlyStopping counter: 6 out of 15\n",
            "Epoch 64: train_loss = 0.3972, val_loss = 0.3829\n",
            "EarlyStopping counter: 7 out of 15\n",
            "Epoch 65: train_loss = 0.3958, val_loss = 0.4049\n",
            "EarlyStopping counter: 8 out of 15\n",
            "Epoch 66: train_loss = 0.3991, val_loss = 0.3940\n",
            "EarlyStopping counter: 9 out of 15\n",
            "Epoch 67: train_loss = 0.4002, val_loss = 0.4136\n",
            "EarlyStopping counter: 10 out of 15\n",
            "Epoch 68: train_loss = 0.4014, val_loss = 0.4082\n",
            "Validation loss decreased (0.382395 --> 0.380689).  Saving model ...\n",
            "Epoch 69: train_loss = 0.3974, val_loss = 0.3807\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 70: train_loss = 0.3976, val_loss = 0.4318\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 71: train_loss = 0.3988, val_loss = 0.3822\n",
            "EarlyStopping counter: 3 out of 15\n",
            "Epoch 72: train_loss = 0.3981, val_loss = 0.3829\n",
            "Validation loss decreased (0.380689 --> 0.379597).  Saving model ...\n",
            "Epoch 73: train_loss = 0.3956, val_loss = 0.3796\n",
            "Validation loss decreased (0.379597 --> 0.375941).  Saving model ...\n",
            "Epoch 74: train_loss = 0.3971, val_loss = 0.3759\n",
            "EarlyStopping counter: 1 out of 15\n",
            "Epoch 75: train_loss = 0.3971, val_loss = 0.3839\n",
            "EarlyStopping counter: 2 out of 15\n",
            "Epoch 76: train_loss = 0.3979, val_loss = 0.3823\n",
            "EarlyStopping counter: 3 out of 15\n",
            "Epoch 77: train_loss = 0.3977, val_loss = 0.3898\n",
            "EarlyStopping counter: 4 out of 15\n",
            "Epoch 78: train_loss = 0.3947, val_loss = 0.3853\n",
            "EarlyStopping counter: 5 out of 15\n",
            "Epoch 79: train_loss = 0.3921, val_loss = 0.3939\n",
            "EarlyStopping counter: 6 out of 15\n",
            "Epoch 80: train_loss = 0.3950, val_loss = 0.3828\n",
            "EarlyStopping counter: 7 out of 15\n",
            "Epoch 81: train_loss = 0.3943, val_loss = 0.3778\n",
            "EarlyStopping counter: 8 out of 15\n",
            "Epoch 82: train_loss = 0.3921, val_loss = 0.3820\n",
            "EarlyStopping counter: 9 out of 15\n",
            "Epoch 83: train_loss = 0.3914, val_loss = 0.3844\n",
            "EarlyStopping counter: 10 out of 15\n",
            "Epoch 84: train_loss = 0.3965, val_loss = 0.3770\n",
            "EarlyStopping counter: 11 out of 15\n",
            "Epoch 85: train_loss = 0.3936, val_loss = 0.3807\n",
            "EarlyStopping counter: 12 out of 15\n",
            "Epoch 86: train_loss = 0.3921, val_loss = 0.3786\n",
            "EarlyStopping counter: 13 out of 15\n",
            "Epoch 87: train_loss = 0.3946, val_loss = 0.3982\n",
            "EarlyStopping counter: 14 out of 15\n",
            "Epoch 88: train_loss = 0.3912, val_loss = 0.4001\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assuming the model and dataset are already defined\n",
        "model = PhishHookNet(input_size=X_train.shape[1])  # Adjust input size based on actual features\n",
        "\n",
        "# Loss function, optimizer, and early stopping\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01) \n",
        "early_stopping = EarlyStopping(patience=15, verbose=True)\n",
        "\n",
        "# Training loop with model saving based on validation loss improvement\n",
        "num_epochs = 100\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss = validate(model, val_loader, criterion)\n",
        "    \n",
        "    # Early stopping\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "    print(f'Epoch {epoch+1}: train_loss = {train_loss:.4f}, val_loss = {val_loss:.4f}')\n",
        "\n",
        "# Load the best model back\n",
        "model.load_state_dict(torch.load('phishing_url_model.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KqUCy3isd23"
      },
      "source": [
        "## Extract URL Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "zwE5KArvsfOA"
      },
      "outputs": [],
      "source": [
        "def extract_url_features(url):\n",
        "    features = {}\n",
        "    parsed_url = urlparse(url)\n",
        "    query_params = parse_qs(parsed_url.query)\n",
        "\n",
        "    # Basic features from URL components\n",
        "    features['length_url'] = len(url)\n",
        "    features['length_hostname'] = len(parsed_url.netloc)\n",
        "    features['ip'] = 1 if parsed_url.hostname and parsed_url.hostname.replace('.', '').isdigit() else 0\n",
        "    features['nb_dots'] = url.count('.')\n",
        "    features['nb_hyphens'] = url.count('-')\n",
        "    features['nb_at'] = url.count('@')\n",
        "    features['nb_qm'] = url.count('?')\n",
        "    features['nb_and'] = url.count('&')\n",
        "    features['nb_or'] = url.count('|')\n",
        "    features['nb_eq'] = len(query_params)\n",
        "    features['nb_underscore'] = url.count('_')\n",
        "    features['nb_tilde'] = url.count('~')\n",
        "    features['nb_percent'] = url.count('%')\n",
        "    features['nb_slash'] = url.count('/')\n",
        "    features['nb_star'] = url.count('*')\n",
        "    features['nb_colon'] = url.count(':')\n",
        "    features['nb_comma'] = url.count(',')\n",
        "    features['nb_semicolumn'] = url.count(';')\n",
        "    features['nb_dollar'] = url.count('$')\n",
        "    features['nb_space'] = url.count(' ')\n",
        "    features['nb_www'] = parsed_url.netloc.count('www')\n",
        "    features['nb_com'] = parsed_url.netloc.count('.com')\n",
        "    features['nb_dslash'] = url.count('//')\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Model with A Different Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the new dataset: 64.35%\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "test_dataset = pd.read_csv(\"test_phishing_data.csv\")\n",
        "\n",
        "def preprocess_data(dataset):\n",
        "    # Convert labels from 'good'/'bad' to 0/1\n",
        "    dataset['Label'] = (dataset['Label'] == 'bad').astype(int)\n",
        "\n",
        "    # Extract features for each URL\n",
        "    features = [extract_url_features(url) for url in dataset['URL']]\n",
        "    feature_df = pd.DataFrame(features)\n",
        "\n",
        "    return feature_df, dataset['Label']\n",
        "\n",
        "features, labels = preprocess_data(test_dataset)\n",
        "\n",
        "# Convert to tensors\n",
        "features_tensor = torch.tensor(features.values, dtype=torch.float32)\n",
        "labels_tensor = torch.tensor(labels.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "test_dataset = TensorDataset(features_tensor, labels_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            predicted = outputs.round()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "model.eval()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = evaluate_model(model, test_loader)\n",
        "print(f'Accuracy of the model on the new dataset: {accuracy * 100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
