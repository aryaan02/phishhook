{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVa8R4nWyAy_"
      },
      "source": [
        "# PhishHook - A Phishing URL Detector - UCI Dataset\n",
        "#### By: Aryaan Khan and Bradley Lewis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iZYwNqUydlU"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "05wY1ux9x9D7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from phishhooknet import PhishHookNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-NbEyT20IkA"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo \n",
        "  \n",
        "# fetch dataset \n",
        "phiusiil_phishing_url_website = fetch_ucirepo(id=967) \n",
        "  \n",
        "# data (as pandas dataframes) \n",
        "X = phiusiil_phishing_url_website.data.features \n",
        "y = phiusiil_phishing_url_website.data.targets \n",
        "\n",
        "feature_names = [\n",
        "    'URLLength', 'DomainLength', 'IsDomainIP',\n",
        "    'TLDLength', 'NoOfSubDomain', 'HasObfuscation', 'IsHTTPS', 'NoOfEqualsInURL',\n",
        "    'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL',\n",
        "    'SpacialCharRatioInURL', 'NoOfLettersInURL', 'LetterRatioInURL',\n",
        "    'NoOfDegitsInURL', 'DegitRatioInURL'\n",
        "]\n",
        "\n",
        "# Select only the columns that are included in the feature_names list\n",
        "X = X[feature_names]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR3DtFf12Ize"
      },
      "source": [
        "## Create the Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_J6ug_ZA2LXF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MPS is available!\n",
            "URLLength                     0\n",
            "DomainLength                  0\n",
            "IsDomainIP                    0\n",
            "TLDLength                     0\n",
            "NoOfSubDomain                 0\n",
            "HasObfuscation                0\n",
            "IsHTTPS                       0\n",
            "NoOfEqualsInURL               0\n",
            "NoOfQMarkInURL                0\n",
            "NoOfAmpersandInURL            0\n",
            "NoOfOtherSpecialCharsInURL    0\n",
            "SpacialCharRatioInURL         0\n",
            "NoOfLettersInURL              0\n",
            "LetterRatioInURL              0\n",
            "NoOfDegitsInURL               0\n",
            "DegitRatioInURL               0\n",
            "dtype: int64\n",
            "torch.Size([141477, 16]) torch.Size([141477, 1])\n",
            "torch.Size([47159, 16]) torch.Size([47159, 1])\n",
            "torch.Size([47159, 16]) torch.Size([47159, 1])\n"
          ]
        }
      ],
      "source": [
        "# Check if MPS is available\n",
        "if torch.backends.mps.is_available():\n",
        "    print(\"MPS is available!\")\n",
        "    # Set the device to MPS\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    print(\"CUDA is available!\")\n",
        "    # Set the device to CUDA\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA not available. Using CPU instead.\")\n",
        "    # Set the device to CPU\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Split the data into temporary training data and final test data\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the temporary training data into final training data and validation data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
        "\n",
        "# Convert all columns to float (adjust based on your specific data needs)\n",
        "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
        "X_val = X_val.apply(pd.to_numeric, errors='coerce')\n",
        "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Check for any remaining non-convertible items\n",
        "print(X_train.isnull().sum())\n",
        "\n",
        "# Handle missing values if there are any\n",
        "X_train = X_train.fillna(0)\n",
        "X_val = X_val.fillna(0)\n",
        "X_test = X_test.fillna(0)\n",
        "\n",
        "# Convert data to tensors and transfer to the specified device\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
        "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1).to(device)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "# Create Tensor datasets for all sets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoaders for all datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=30000, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=10000, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10000, shuffle=False)\n",
        "\n",
        "# Confirm the DataLoader details\n",
        "print(train_loader.dataset.tensors[0].shape, train_loader.dataset.tensors[1].shape)\n",
        "print(val_loader.dataset.tensors[0].shape, val_loader.dataset.tensors[1].shape)\n",
        "print(test_loader.dataset.tensors[0].shape, test_loader.dataset.tensors[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBNkBiOgyqpW"
      },
      "source": [
        "## Define the Training and Validation Loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3MsmK3vP2ggw"
      },
      "outputs": [],
      "source": [
        "# Function to train the model\n",
        "def train(model, train_loader, optimizer, criterion):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "# Function to validate the model\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            predicted = outputs.round()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    avg_loss = total_loss / len(val_loader.dataset)\n",
        "    accuracy = correct / total * 100\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Early Stopping Condition Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'phishing_uci_url_model.pth')\n",
        "        self.val_loss_min = val_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgfYAE0822ML"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Bkekk9wt21hg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss decreased (inf --> 0.756463).  Saving model ...\n",
            "Epoch 1: train_loss = 1.9414, val_loss = 0.7565\n",
            "Validation loss decreased (0.756463 --> 0.662974).  Saving model ...\n",
            "Epoch 2: train_loss = 0.6999, val_loss = 0.6630\n",
            "Validation loss decreased (0.662974 --> 0.510339).  Saving model ...\n",
            "Epoch 3: train_loss = 0.6215, val_loss = 0.5103\n",
            "Validation loss decreased (0.510339 --> 0.382006).  Saving model ...\n",
            "Epoch 4: train_loss = 0.4888, val_loss = 0.3820\n",
            "Validation loss decreased (0.382006 --> 0.367797).  Saving model ...\n",
            "Epoch 5: train_loss = 0.4172, val_loss = 0.3678\n",
            "Validation loss decreased (0.367797 --> 0.296566).  Saving model ...\n",
            "Epoch 6: train_loss = 0.3573, val_loss = 0.2966\n",
            "Validation loss decreased (0.296566 --> 0.214582).  Saving model ...\n",
            "Epoch 7: train_loss = 0.2902, val_loss = 0.2146\n",
            "Validation loss decreased (0.214582 --> 0.135098).  Saving model ...\n",
            "Epoch 8: train_loss = 0.2204, val_loss = 0.1351\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 9: train_loss = 0.1484, val_loss = 0.1557\n",
            "Validation loss decreased (0.135098 --> 0.080711).  Saving model ...\n",
            "Epoch 10: train_loss = 0.1468, val_loss = 0.0807\n",
            "Validation loss decreased (0.080711 --> 0.044463).  Saving model ...\n",
            "Epoch 11: train_loss = 0.0693, val_loss = 0.0445\n",
            "Validation loss decreased (0.044463 --> 0.027562).  Saving model ...\n",
            "Epoch 12: train_loss = 0.0438, val_loss = 0.0276\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 13: train_loss = 0.0312, val_loss = 0.0273\n",
            "Validation loss decreased (0.027562 --> 0.025580).  Saving model ...\n",
            "Epoch 14: train_loss = 0.0289, val_loss = 0.0256\n",
            "Validation loss decreased (0.025580 --> 0.024019).  Saving model ...\n",
            "Epoch 15: train_loss = 0.0251, val_loss = 0.0240\n",
            "Validation loss decreased (0.024019 --> 0.021641).  Saving model ...\n",
            "Epoch 16: train_loss = 0.0238, val_loss = 0.0216\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 17: train_loss = 0.0223, val_loss = 0.0212\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 18: train_loss = 0.0211, val_loss = 0.0207\n",
            "Validation loss decreased (0.021641 --> 0.019487).  Saving model ...\n",
            "Epoch 19: train_loss = 0.0204, val_loss = 0.0195\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 20: train_loss = 0.0201, val_loss = 0.0190\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 21: train_loss = 0.0194, val_loss = 0.0189\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 22: train_loss = 0.0191, val_loss = 0.0189\n",
            "Validation loss decreased (0.019487 --> 0.018374).  Saving model ...\n",
            "Epoch 23: train_loss = 0.0182, val_loss = 0.0184\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 24: train_loss = 0.0176, val_loss = 0.0179\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 25: train_loss = 0.0175, val_loss = 0.0178\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 26: train_loss = 0.0170, val_loss = 0.0182\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 27: train_loss = 0.0168, val_loss = 0.0188\n",
            "Validation loss decreased (0.018374 --> 0.017203).  Saving model ...\n",
            "Epoch 28: train_loss = 0.0169, val_loss = 0.0172\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 29: train_loss = 0.0163, val_loss = 0.0174\n",
            "Validation loss decreased (0.017203 --> 0.015769).  Saving model ...\n",
            "Epoch 30: train_loss = 0.0163, val_loss = 0.0158\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch 31: train_loss = 0.0160, val_loss = 0.0154\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch 32: train_loss = 0.0157, val_loss = 0.0153\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch 33: train_loss = 0.0155, val_loss = 0.0161\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch 34: train_loss = 0.0152, val_loss = 0.0160\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "# Assuming the model and dataset are already defined\n",
        "model = PhishHookNet(input_size=X_train.shape[1]).to(device)  # Adjust input size based on actual features\n",
        "\n",
        "# Loss function, optimizer, and early stopping\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.001)\n",
        "\n",
        "# Training loop with model saving based on validation loss improvement\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    val_loss, accuracy = validate(model, val_loader, criterion)\n",
        "    \n",
        "    # Early stopping\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n",
        "\n",
        "    print(f'Epoch {epoch+1}: train_loss = {train_loss:.4f}, val_loss = {val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training accuracy: 99.74%\n",
            "Validation accuracy: 99.68%\n",
            "Test accuracy: 99.74%\n"
          ]
        }
      ],
      "source": [
        "# Load the best model back\n",
        "model.load_state_dict(torch.load('phishing_uci_url_model.pth'))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "model.eval()\n",
        "\n",
        "# Calculate accuracy\n",
        "train_accuracy = validate(model, train_loader, criterion)[1]\n",
        "val_accuracy = validate(model, val_loader, criterion)[1]\n",
        "test_accuracy = validate(model, test_loader, criterion)[1]\n",
        "\n",
        "# Print the results\n",
        "print(f'Training accuracy: {train_accuracy:.2f}%')\n",
        "print(f'Validation accuracy: {val_accuracy:.2f}%')\n",
        "print(f'Test accuracy: {test_accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
